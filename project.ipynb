{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from collections.abc import Iterable\n",
    "from datasets import load_dataset, list_datasets\n",
    "\n",
    "# Model and tokenizer from ðŸ¤— Transformers\n",
    "from transformers import AutoModelForSequenceClassification, \\\n",
    "    BertForSequenceClassification, BertTokenizerFast, AutoModel, AutoTokenizer\n",
    "\n",
    "# Code you will write for this assignment\n",
    "from train_model import init_model, preprocess_dataset, init_trainer\n",
    "from test_model import init_tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hate_speech_offensive (/Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d38193aae345668f3f0c65bec5726c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-3a9bf3a1049559c7.arrow and /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-69dfaebdf31e0aae.arrow\n",
      "Loading cached split indices for dataset at /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-b2a34171e8c0b921.arrow and /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-38b528465638c7b4.arrow\n"
     ]
    }
   ],
   "source": [
    "hate_speech = load_dataset(\"hate_speech_offensive\")\n",
    "split = hate_speech[\"train\"].train_test_split(.1, seed=3463)\n",
    "hate_speech[\"train\"] = split[\"train\"]\n",
    "hate_speech[\"test\"] = split[\"test\"]\n",
    "\n",
    "split = hate_speech[\"train\"].train_test_split(.1, seed=3463)\n",
    "hate_speech[\"train\"] = split[\"train\"]\n",
    "hate_speech[\"val\"] = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a9993e32244314a9490ca9ed5368b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d24cea7357456cbc5cf44ddcc13f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda3afaac4684616b935be275d2b70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da615d311c747c9999f0d2473d16654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104c46d30d5249f089e34caa73ca8c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2c7ff08faa473bbd48cacf1f710edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:\n",
      "<class 'list'>\n",
      "[3, 3]\n",
      "\n",
      "hate_speech_count:\n",
      "<class 'list'>\n",
      "[0, 0]\n",
      "\n",
      "offensive_language_count:\n",
      "<class 'list'>\n",
      "[3, 3]\n",
      "\n",
      "neither_count:\n",
      "<class 'list'>\n",
      "[0, 0]\n",
      "\n",
      "class:\n",
      "<class 'list'>\n",
      "[1, 1]\n",
      "\n",
      "tweet:\n",
      "<class 'list'>\n",
      "['@CinWicked Did you j', 'Fuck bitches get mon']\n",
      "\n",
      "class_converted:\n",
      "<class 'list'>\n",
      "[1, 1]\n",
      "\n",
      "input_ids:\n",
      "<class 'list'>\n",
      "[[0, 5238, 29442, 22972, 689, 14, 45, 329, 122, 4256, 9893, 3866, 11, 1287, 50239, 41407, 50239, 12, 2, 1], [0, 1004, 1179, 51, 325, 4056, 43027, 515, 12, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "\n",
      "token_type_ids:\n",
      "<class 'list'>\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "attention_mask:\n",
      "<class 'list'>\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_speech[\"train\"] = preprocess_dataset(hate_speech[\"train\"], tokenizer)\n",
    "hate_speech[\"val\"] = preprocess_dataset(hate_speech[\"val\"], tokenizer)\n",
    "hate_speech[\"test\"] = preprocess_dataset(hate_speech[\"test\"], tokenizer)\n",
    "\n",
    "# Visualize the preprocessed dataset\n",
    "for k, v in hate_speech[\"val\"][:2].items(): \n",
    "    print(\"{}:\\n{}\\n{}\\n\".format(k, type(v),\n",
    "                                 [item[:20] if isinstance(item, Iterable) else \n",
    "                                 item for item in v[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/junzeli/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/junzeli/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'lm_head.decoder.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'lm_head.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'classifier.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = init_trainer(\"vinai/bertweet-base\", hate_speech[\"train\"], hate_speech[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def read_result(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        object = pickle.load(f)\n",
    "    return object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.4309874,  2.5544624],\n",
       "       [-3.5020423,  3.4747117],\n",
       "       [ 1.2080353, -1.0986334],\n",
       "       ...,\n",
       "       [-3.4175987,  3.4315543],\n",
       "       [-3.4963927,  3.4849124],\n",
       "       [-3.5263796,  3.495295 ]], dtype=float32), label_ids=None, metrics={'test_runtime': 374.4704, 'test_samples_per_second': 6.62, 'test_steps_per_second': 0.828})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_result('test_results.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bece8a4ae55facad4a9f75626b6157080e84d45cc8667e9d052a17ecde009f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
