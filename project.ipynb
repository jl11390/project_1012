{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from collections.abc import Iterable\n",
    "from datasets import load_dataset, list_datasets\n",
    "\n",
    "# Model and tokenizer from ðŸ¤— Transformers\n",
    "from transformers import AutoModelForSequenceClassification, \\\n",
    "    BertForSequenceClassification, BertTokenizerFast, AutoModel, AutoTokenizer\n",
    "\n",
    "# Code you will write for this assignment\n",
    "from train_model import init_model, preprocess_dataset, init_trainer\n",
    "from test_model import init_tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hate_speech_offensive (/Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9afbf6dbac443eb7e67fdc2334cdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-3a9bf3a1049559c7.arrow and /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-69dfaebdf31e0aae.arrow\n",
      "Loading cached split indices for dataset at /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-b2a34171e8c0b921.arrow and /Users/junzeli/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-38b528465638c7b4.arrow\n"
     ]
    }
   ],
   "source": [
    "hate_speech = load_dataset(\"hate_speech_offensive\")\n",
    "split = hate_speech[\"train\"].train_test_split(.1, seed=3463)\n",
    "hate_speech[\"train\"] = split[\"train\"]\n",
    "hate_speech[\"test\"] = split[\"test\"]\n",
    "\n",
    "split = hate_speech[\"train\"].train_test_split(.1, seed=3463)\n",
    "hate_speech[\"train\"] = split[\"train\"]\n",
    "hate_speech[\"val\"] = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db3888cbc434f7e97c5115a615fa332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfd909d883f42a3bbad7a8cd11731a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d38eccd5f34e9abc5ea2e2c78aa93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe33d22ec2a4dfc818276c02ed29901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4a35c9af524826bcd6e6ed65ed92dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d920c33e68b4790bd49f903f08abe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:\n",
      "<class 'list'>\n",
      "[3, 3]\n",
      "\n",
      "hate_speech_count:\n",
      "<class 'list'>\n",
      "[0, 0]\n",
      "\n",
      "offensive_language_count:\n",
      "<class 'list'>\n",
      "[3, 3]\n",
      "\n",
      "neither_count:\n",
      "<class 'list'>\n",
      "[0, 0]\n",
      "\n",
      "class:\n",
      "<class 'list'>\n",
      "[1, 1]\n",
      "\n",
      "tweet:\n",
      "<class 'list'>\n",
      "['@CinWicked Did you j', 'Fuck bitches get mon']\n",
      "\n",
      "class_converted:\n",
      "<class 'list'>\n",
      "[1, 1]\n",
      "\n",
      "input_ids:\n",
      "<class 'list'>\n",
      "[[0, 5238, 29442, 22972, 689, 14, 45, 329, 122, 4256, 9893, 3866, 11, 1287, 50239, 41407, 50239, 12, 2, 1], [0, 1004, 1179, 51, 325, 4056, 43027, 515, 12, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "\n",
      "token_type_ids:\n",
      "<class 'list'>\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "attention_mask:\n",
      "<class 'list'>\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "labels:\n",
      "<class 'list'>\n",
      "[1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_speech[\"train\"] = preprocess_dataset(hate_speech[\"train\"], tokenizer)\n",
    "hate_speech[\"val\"] = preprocess_dataset(hate_speech[\"val\"], tokenizer)\n",
    "hate_speech[\"test\"] = preprocess_dataset(hate_speech[\"test\"], tokenizer)\n",
    "\n",
    "# Visualize the preprocessed dataset\n",
    "for k, v in hate_speech[\"val\"][:2].items(): \n",
    "    print(\"{}:\\n{}\\n{}\\n\".format(k, type(v),\n",
    "                                 [item[:20] if isinstance(item, Iterable) else \n",
    "                                 item for item in v[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/junzeli/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/junzeli/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'lm_head.decoder.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'lm_head.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'classifier.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = init_trainer(\"vinai/bertweet-base\", hate_speech[\"train\"], hate_speech[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def read_result(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        object = pickle.load(f)\n",
    "    return object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = read_result('test_results.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2479"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "texts = np.array(hate_speech['test']['tweet'])\n",
    "true_labels = np.array(hate_speech['test']['labels'])\n",
    "len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2479"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = obj[0].argmax(axis=1)\n",
    "len(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Up early then a bitch driving to denton omg can I move already',\n",
       "       '\"@nohands_24: \"@20ToLife_: \"@nohands_24: \"@DejaaLeeann_: @B_Coleman2 swears I\\'m #1 man\" easily\" easily\" behind Austin\" nip?',\n",
       "       \"@TheIronPony You rebel. Next you'll be ripping mattress tags off. It's a slippery slope.\",\n",
       "       'This bitch @Lvl_7_Eevee buys sparkling water.',\n",
       "       \"lmaoooo RT @PhillyTheBoss: Everything on BET is trash. RT @MrCOOK_: B-Rad RT @fkinLIVE: White Mike was on BET, he can't be trash\",\n",
       "       'Sixten just shoved like 8 Oreos down his throat',\n",
       "       \"&#8220;@CommonManKFAN: And that's why mock drafts are a joke my friends.....&#8221;Anyone???\",\n",
       "       'RT @briangaar: Old white people complaining about government tyranny is like the Yankees being upset that players make too much money',\n",
       "       'Are poor little guinea pig died :(',\n",
       "       'Finally got my medicine and crackers!',\n",
       "       'Joba Chamberlain looks like a homeless guy that won a contest. #Yankees',\n",
       "       \"@eGoTheIcoN everytime pro teams do GB tourneys that get schmanged. Im not fanboying im just saying I don't think u should say they're trash.\",\n",
       "       'RT @Kstar__: Told you RT @Astronaut_Tone: Artprize is more than trash',\n",
       "       '@Hendrixoxo lemme guess you wore the hilly hicks sweats/leggings &#128514;&#128514;&#127874;&#127856;&#128079;&#127825;',\n",
       "       'lol @ that tweet zebra.',\n",
       "       'RT @LuvKittensDaily: Oreo being adorable http://t.co/5ZMZVT0sUl',\n",
       "       '&#128530; my nerves are being worked cos of my monkey &#128529;&#128545;',\n",
       "       'This store is SO redneck...', \"I'll give you a baby zebra girl.\",\n",
       "       \"Stop rt'ing Celtics trash please\",\n",
       "       \"RT @SullivanMcPig: If those Captcha pictures get any more fuzzy I'm going to need a bot to decipher them\",\n",
       "       \"RT @EJ_RealTalk: &#8220;@SirRayCharles: Mo Claiborne is so trash&#8221;I can't say it enough fam &#128530;\",\n",
       "       'Why no boycott of racist \"redskins\"?\\n#Redskins\\n#ChangeTheName\\n#ChangeTheMascot',\n",
       "       \"@intheKCmix The last planet of the apes with my kids... It's been a minute\",\n",
       "       '@QueenSAVage97 naw I Juss like making you a lil mad &#128514; I have been gone for too long tho I hate the boonies&#128589;&#128564;',\n",
       "       '@britty_jurgens a music suggestion is \"lil spook\"',\n",
       "       'I thought she was albino tho &#128563;',\n",
       "       \"Oh snap I'm just realizing this skinny tranny is Jared Leto. He's Wo unrecognizable with all that weight loss. Geez!!!\",\n",
       "       'Do you know where I grew up Skippy? @mouzone_brother @psherm07 @EchoesErrant @gigg423 @jstines3',\n",
       "       'RT @NFL_Memes: Buckwheat and Porky http://t.co/bo96nelS2P',\n",
       "       '@PWilliamsD I need some clams on the half shell :-). Down here all you can get is deep fried clam necks.',\n",
       "       '@justinhall3434 \\nGod bless you whitey. I be sleepin in tomorrow',\n",
       "       'RT @Yankees: #Yankees win, 6-5, on a #walkoff single by Derek Jeter!!!!!! #FarewellCaptain',\n",
       "       \"RT @InNewZealand: The Knicks are still trash, Melo signed for the money he don't even care about being on a good winning team.\",\n",
       "       \"Some Douche cubs redneck cubs fan freaked out asking if bites..... No you bloody BERK!!! He's like the&#8230; http://t.co/ulc6CMVtki\",\n",
       "       \"RT @OutTheOrdinary1: @P0WERS_ @eGoaTizM @Blaztful powers U don't go outside ur tanks been on full for 2 months U anti social monkey\",\n",
       "       'RT @kristinhersh: my son, Wyatt: \"do rednecks have mullets to keep from getting red necks?\"',\n",
       "       'sheryl crow being the bae',\n",
       "       \"@SarahR_82 \\nY'all gots to take da colored vacation. Kentucky fried chicken but only da white meat\",\n",
       "       'I really want a girl to make me some brownies &#128553;&#128557;',\n",
       "       '\"Bae\" sounds like such a ghetto word. Use something else',\n",
       "       '@OldManRo you should check it out... it aint animated and has Charlie Murphy in it',\n",
       "       '@RonanFarrow Yeah we know about Anglo-American principles.#slavery #hate #racism',\n",
       "       'RT @Blessedyaszay: #IWantToPunchPeopleWho mock God',\n",
       "       \"nope RT @DerekIsNormal: Just ordered Domino's. Am I trash?\",\n",
       "       '@elchavaloko @Buckm00se yeah sorry monkey team just keeps kicking ppl, waiting for muh pizza',\n",
       "       '1300 for the eggs sunny side nicca .. @EastsideDame',\n",
       "       'So my light was off &amp; door was closed , mom walks in turn lights on . Tells me to take trash out in the morning . then walked out.. Light on',\n",
       "       '@mark_mac18 @sheppast @rianscalia groves is trash IMO',\n",
       "       'Practically got arrested tonight, not allowed on UTampa grounds for a year now... fml im retarded',\n",
       "       '@kincade00 @mistaturk5 lmaooo just call my phone if you want them birds *Migos Voice*',\n",
       "       '@Briccone79 @pvvrivierenland ja moest van plisie niet meteen wild gaan denken.Juistja,willen wel meer mensen mij vertellen hoe k moet denken',\n",
       "       'My teacher always says \"Has a man ever abused his wife while using marijuana? Not unless she hides the Oreo cookies then she deserves it.\"',\n",
       "       '@dish \\nWas just wondering if yo espect to add mo colored tv stations befo I renew dis',\n",
       "       'before and after I started playing flappy bird ...\\nbefore: &#128522;&#127800;&#128077;&#9728;&#65039;\\nafter: &#128545;&#128299;&#128165;&#128163;&#128293;&#128659;&#128658;',\n",
       "       '\" momma said no pussy cats inside my doghouse \"',\n",
       "       '@RonnieMack12 dawg, you so trash your momma got charged with littering when she dropped you off for school',\n",
       "       'RT @ryancruz_: Travis always on my TL being ghetto &#128514;&#128514;',\n",
       "       'I feel utterly retarded. Thanks big brother&#128563;&#128514; @Crawford96M',\n",
       "       'Bought this Lp in Manchester the other week, had no idea it was coloured vinyl. Man what a steal!!!!! #VinylGasm #Re-Machined',\n",
       "       '@LILNTHEBASEDGOD her face ugly to me, &amp; her nudes were trash.',\n",
       "       \"Mann are first game is 3 weeks from now can't wait - once a redskin always a redskin -\",\n",
       "       \"Now I'm listening to lil spook getting the good kinda sad Imma be single this fall I know you'll getting annoyed with me on your TL but aye\",\n",
       "       \"RT @grind2times: I don't think I even been in a real relationship...i thought they was real but they were just trash\",\n",
       "       'Bruh... @theofficialeg10 soo trash....',\n",
       "       \"RT @AyMrCarter: Apologies trash.. What's understood ain't gotta be explained\",\n",
       "       '\"@FunnyPicsDepot: this the \"I play soccer, cheat on girls, and wear khaki coloured cargos\" haircut http://t.co/ZUai7qWBIR\" &#128514; yup',\n",
       "       'RT @badnradbrad: @whattheflocka lol have u seen when ike and his kindergarten teacher start having a sexual relationship',\n",
       "       '@TreyDoe_: @HallDerik @20ToLife_ once fag.\" You got pressed to the ground? ..hang up your cleats n never put them back on or think about it',\n",
       "       \"Don't follow the astros they said. They're trash they said. Well now look at them #astros\",\n",
       "       \"@Lvl_7_Eevee @UnicornCowboy @hisorraya I'll trade you one car drift for brownies.\",\n",
       "       \"@BlackManUSA @chrislhayes well that's some cracker jack secret service work there. jfk anyone?\",\n",
       "       'RT @TheBananaFacts: wiz khalifas\\'s song \"black and yellow\" is a beautiful song about a bruised banana',\n",
       "       \"RT @FriendlyAssh0le: Todays Horoscope: \\n\\nyou're still a hoe\\n\\nThat is all. have a nice day\",\n",
       "       \"@brittbritt_naay &#128514; I'm a wexican\",\n",
       "       \"yea so about @N_tel 's new friend.. all my friends kno they're only allowed one cute darkie friend n that's me! lol\",\n",
       "       '@KingLuger carne asada, Gallo pinto, tajadas verde, queso frito and a salad With milca',\n",
       "       \"How long are they going to let the fools in #Ferguson continue? Stop pussy footin' &amp; disperse them by WHATEVER means necessary!\",\n",
       "       \"@Scoob3y yeah and you're gonna play with retards who wanna do drugs all day but &#128056;&#9749;&#65039;\",\n",
       "       'Inbed eating left over poppadom. #livinthedream',\n",
       "       \"@RetroReasoning I think most RPGs are for the birds. I ain't no bird brain yo\",\n",
       "       \"I hopes dis don't come as a severe shock to use but I'm colored\",\n",
       "       \"@rachel_gautreau @babyydrey hey Rachel G my name isint Kenneth ok let's just get that straight &#128070; you best be backing off before I go ghetto\",\n",
       "       'They say our state bird is rare...I literally see one at least once a day',\n",
       "       'You said you be smoking strong but your wrong you been smoking trash and it aint in my lungs if it aint that pressure',\n",
       "       '10 RT @ParadoxicalMike 1.5 RT @iDavey On average, 2. But I can be an 8 when need be. \"@SinsOfMyFather_ On a scale of 1-10 how ghetto are u?\"',\n",
       "       '@ItsMeGrizz dink now uknow thats not true',\n",
       "       'At pussy cat lounge killin it with my girlz @blaqdior and @hpmiss',\n",
       "       'That crow has socks',\n",
       "       'Thanks to @JoMousley80 cracker tweets today, my pilchards on Krackerwheat are repeating on me like a typical night on the #BBC :-/',\n",
       "       '\"@QUAN1T0: 61% of welfare/government aid is claimed by white people. So y\\'all black slander is trash now.\"',\n",
       "       'My doctors office is retarded, \"we noticed you put in a prescription order, so we canceled it. Give us a call if you need a refill, thanks!\"',\n",
       "       \"@IAmPikey \\nI wonders why dare ain't no colored folk up dare?\",\n",
       "       \"@dminion25 it's kinda screwed up that they were my guinea pigs but i was only experimenting on taste instead i got more interesting results.\",\n",
       "       'RT @godlevelshit510: Why Adonnis look like he bout to sell the most ghetto used cars of 2014 ? http://t.co/icjCf6km8V',\n",
       "       'Char-broiled #cod, yellow #rice, saut&#233;ed spinach and artichoke hearts. Made with love for @voxpulchra.&#8230; http://t.co/vWjWlghRGO',\n",
       "       \"This reminds me of another quality video lol. He ain't fittin to front for these peckerwoods! http://t.co/AYpwHr0J62\",\n",
       "       'Never turned my back on the birds',\n",
       "       '&#8220;@BeautyOfASinner: So they really deleted flappy bird from the App Store cus someone killed themselves over it &#128514;&#8221; foreal foreal ?? &#128561;&#128557;',\n",
       "       'Peace my friend..Only Peace..I am not your enemy..Dont say that word again..Look at my Facebook wall..One of my best friends ever was a Moor',\n",
       "       '\"@Addicted2Guys: -SimplyAddictedToGuys http://t.co/1jL4hi8ZMF\" woof woof hot scally lad',\n",
       "       'RT @FunnyJokeBook: I hope I made these pot brownies right http://t.co/iDDDHPPjiu',\n",
       "       'Cheer up Charlie.',\n",
       "       'Unintended consequences of the current fed regulations. \\nTrays of uneaten food thrown in the trash, \\nHungry kids.\\n http://t.co/aaMJhHOW7y',\n",
       "       '@Misters206 You look like a monkey',\n",
       "       \"RT @AvengerStratus: There's nothing hotter than a pale albino chick who can play guitar.\",\n",
       "       \"@michigannews13 wow. Thats great language coming from a HS coach, you sure are a role model, you're trash, just like your account\",\n",
       "       '@TheShoeBible @Steamed_Greens Them hoes are clean!',\n",
       "       '@steve_phillips_ funny, I thought @bobmuellerwkrn #tbt 90s photo this week would have you hanging me over a trash can by my ankles',\n",
       "       \"RT @radiomaru: nerds are cool because they get really judge-y about other nerds being passionate about things they aren't passionate abo ...\",\n",
       "       \"Y'all suck on flappy bird\",\n",
       "       \"Dat da bad thing bout being colored. Day bring me in to fight but don't vite me to da victory partay\",\n",
       "       'Time to start listing to il spook',\n",
       "       'There is no such rule as \"tie goes to the runner\". Out or safe. #Yankees #JohnSterling',\n",
       "       '#metal only, honkies!',\n",
       "       'RT @it_itt: @docrocktex26 doing nothing as an agenda. unbelievable that the Dems failed to use that against the teabaggers. @AfricanPrinces&#8230;',\n",
       "       'RT @losthistories: DC Fri 1/3: Bengali Harlem &amp; Lost Histories South Asian Am reading/convo @busboysandpoets 14th St cc: @SmithsonianAPA @D&#8230;',\n",
       "       'RT @daliasza: &#8220;@NoChillPaz: Me when somebody says Childish Gambino is trash http://t.co/8ejOHDLdD9&#8221;',\n",
       "       \"@Fru_Iam I seen sum of it but I'm tryin see lox n mook\",\n",
       "       'RT @BiertempfelTrib: One more from @JimBowdenESPNxm: \"Yankees wouldn\\'t be letting Polanco sit in minor leagues, because they\\'re all about w&#8230;',\n",
       "       \"RT @Fant4stic63: Can kids say much when their parents pay for everything? RT @Andrea_Michele: If he paying every bill, I can't complain abo&#8230;\",\n",
       "       'is she choking it!! &#128563;&#128563;&#128584;&#128584;&#128553;&#128553;&#8220;@AwwAdorabIe: I want a pet monkey &#128553;&#128525; http://t.co/w7Yr5xKLSC&#8221;',\n",
       "       'Don&#8217;t believe AP&#8217;s ho-hum tone: latest poll is great news for Republicans - Hot Air http://t.co/5HHrFlFP13',\n",
       "       'RT @moosedaugherty: Free bird!!!',\n",
       "       \"Every character on #Thesimpsons is trash. And they're trash on purpose. If you don't get that, you don't get the joke.\",\n",
       "       'RT @IWILLSTILLRISE: If you not gator you gator bait #gatornation',\n",
       "       \"Bjergsen just made a full AS rune page. I don't just me full AS reds and Quints I mean AS reds, blues, yellows, and Quints. 47% AS\",\n",
       "       'RT @MikeTInnes: @AaronWorthing @corrcomm @Icampintense @CarolCNN @CNN So glad Charlie agrees with Carol and CNN that assaulting girls is OK&#8230;'],\n",
       "      dtype='<U342')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[(true_labels == 0) & (pred_labels == 1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bece8a4ae55facad4a9f75626b6157080e84d45cc8667e9d052a17ecde009f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
